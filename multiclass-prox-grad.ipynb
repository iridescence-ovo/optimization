{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9944957-13c1-419a-b2d8-6b932cee05c8",
   "metadata": {},
   "source": [
    "The optimization problem:\n",
    "\t$$\n",
    "\t\\min_{\\beta_0, \\dots, \\beta_9} \\sum_{i=1}^m \\left[ \\log\\left( \\sum_{k=0}^9 \\exp(x_i^\\top \\beta_k) \\right) - \x_i^\\top \\beta_{y_i} \\right] + \\lambda \\sum_{k=0}^9 \\|\\beta}_k\\|_1\n",
    "\t$$\n",
    "\n",
    "which is a multinomial logistic regression with l1-penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomProxGradSolver:\n",
    "    \"\"\"\n",
    "    The solver using proximal gradient descent with problem 1\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, lambda_=0.1, eta=0.1, max_iter=1000, tol=1e-4):\n",
    "        \"\"\"\n",
    "        num_classes: number of classes (default is 10, same as problem 1)\n",
    "        lambda_: regularization term (default 0.1)\n",
    "        eta: the learning rate, I choose the same value for the whole iteration (default=0.01)\n",
    "        max_iter: number of maximum iterations (default 1000)\n",
    "        tol: the tolerance of error (default 1e-4)\n",
    "        beta: the params for the model (all the beta_k, shape is: (num_classes, n_features))\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_ = lambda_\n",
    "        self.lr = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = None\n",
    "\n",
    "    def _softmax(self, Z):\n",
    "        \"\"\"stable softmax calculation, returns pik\"\"\"\n",
    "        # Z =: X @ beta, shape (m, num_classes)\n",
    "        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)  # for numerical stability\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True) # shape: (m, num_classes)\n",
    "\n",
    "    def _compute_loss(self, X, y, probs):\n",
    "        \"\"\"compute the loss\"\"\"\n",
    "        # the loss we aim to minimize: f + g (L1 regularization)\n",
    "        m = X.shape[0]\n",
    "        # calculate the first term using log_likelihood\n",
    "        log_likelihood = -np.log(probs[np.arange(m), y] + 1e-15).sum() # add 1e-15 to avoid log(0)\n",
    "        # calculate the l1 penalty term\n",
    "        l1_penalty = self.lambda_ * np.sum(np.abs(self.beta))\n",
    "        return log_likelihood + l1_penalty\n",
    "\n",
    "    def _compute_gradient(self, X, y, probs):\n",
    "        \"\"\"compute the gradient for f\"\"\"\n",
    "        # X: (m, n), y: (m,), probs: (m, num_classes)\n",
    "        m, n = X.shape\n",
    "        grad = np.zeros_like(self.beta)  # shape: (num_classes, n)\n",
    "        for k in range(self.num_classes):\n",
    "            indicator = (y == k).astype(float)  # shape: (m,)\n",
    "            diff = probs[:, k] - indicator  # shape: (m,)\n",
    "            grad[k] = np.dot(diff, X)  # shape: (n,)\n",
    "        return grad\n",
    "\n",
    "    def _prox_operator(self, beta, threshold):\n",
    "        \"\"\"do the proximal operator\"\"\"\n",
    "        return np.sign(beta) * np.maximum(np.abs(beta) - threshold, 0.0)\n",
    "\n",
    "    def fit(self, X, y, verbose=True):\n",
    "        m, n = X.shape\n",
    "        self.beta = np.zeros((self.num_classes, n))  # Initialize β_k to all zeros\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            logits = X @ self.beta.T  # shape: (m, C)\n",
    "            probs = self._softmax(logits)\n",
    "            loss = self._compute_loss(X, y, probs)\n",
    "            grad = self._compute_gradient(X, y, probs)\n",
    "            beta_old = self.beta.copy()\n",
    "\n",
    "            # Proximal gradient update\n",
    "            for k in range(self.num_classes):\n",
    "                self.beta[k] -= self.lr * grad[k]\n",
    "                self.beta[k] = self._prox_operator(self.beta[k], self.lr * self.lambda_)\n",
    "\n",
    "            # Check convergence\n",
    "            delta = np.linalg.norm(self.beta - beta_old)\n",
    "            if verbose:\n",
    "                print(f\"Iteration {it + 1}, Loss: {loss:.4f}, Δβ: {delta:.6f}\")\n",
    "            if delta < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"using softmax to get pik\"\"\"\n",
    "        logits = X @ self.beta.T\n",
    "        return self._softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"predict new data using trained beta\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a7824b-b9e1-4588-a843-9cd6206b28a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 3308.8148, Δβ: 198.066766\n",
      "Iteration 2, Loss: 6196.5593, Δβ: 41.583865\n",
      "Iteration 3, Loss: 3643.8086, Δβ: 22.782296\n",
      "Iteration 4, Loss: 2290.2070, Δβ: 11.216634\n",
      "Iteration 5, Loss: 1823.8777, Δβ: 8.926595\n",
      "Iteration 6, Loss: 1494.5787, Δβ: 8.506908\n",
      "Iteration 7, Loss: 1320.5094, Δβ: 8.834002\n",
      "Iteration 8, Loss: 1193.1247, Δβ: 8.263413\n",
      "Iteration 9, Loss: 1092.3470, Δβ: 7.427986\n",
      "Iteration 10, Loss: 971.4899, Δβ: 5.436454\n",
      "Iteration 11, Loss: 879.5034, Δβ: 4.242311\n",
      "Iteration 12, Loss: 792.5015, Δβ: 3.895539\n",
      "Iteration 13, Loss: 732.0190, Δβ: 3.587873\n",
      "Iteration 14, Loss: 680.3003, Δβ: 3.524778\n",
      "Iteration 15, Loss: 662.5786, Δβ: 4.233452\n",
      "Iteration 16, Loss: 643.7273, Δβ: 4.835757\n",
      "Iteration 17, Loss: 630.6388, Δβ: 4.589978\n",
      "Iteration 18, Loss: 584.4053, Δβ: 3.463413\n",
      "Iteration 19, Loss: 544.5698, Δβ: 2.351955\n",
      "Iteration 20, Loss: 527.4780, Δβ: 2.063701\n",
      "Iteration 21, Loss: 516.6519, Δβ: 1.954647\n",
      "Iteration 22, Loss: 510.3700, Δβ: 2.013582\n",
      "Iteration 23, Loss: 506.7473, Δβ: 2.258328\n",
      "Iteration 24, Loss: 501.9415, Δβ: 2.449819\n",
      "Iteration 25, Loss: 498.6225, Δβ: 2.350871\n",
      "Iteration 26, Loss: 484.3779, Δβ: 2.099960\n",
      "Iteration 27, Loss: 474.1438, Δβ: 2.092084\n",
      "Iteration 28, Loss: 463.3500, Δβ: 2.244842\n",
      "Iteration 29, Loss: 454.7387, Δβ: 2.407843\n",
      "Iteration 30, Loss: 444.4082, Δβ: 2.471033\n",
      "Iteration 31, Loss: 433.1733, Δβ: 2.397138\n",
      "Iteration 32, Loss: 423.2118, Δβ: 2.264666\n",
      "Iteration 33, Loss: 415.0283, Δβ: 2.286328\n",
      "Iteration 34, Loss: 412.3960, Δβ: 2.396789\n",
      "Iteration 35, Loss: 401.7570, Δβ: 2.286313\n",
      "Iteration 36, Loss: 395.7112, Δβ: 1.663857\n",
      "Iteration 37, Loss: 382.8938, Δβ: 1.281945\n",
      "Iteration 38, Loss: 377.8252, Δβ: 1.345100\n",
      "Iteration 39, Loss: 370.3837, Δβ: 1.185550\n",
      "Iteration 40, Loss: 363.1527, Δβ: 1.129652\n",
      "Iteration 41, Loss: 357.1070, Δβ: 1.159984\n",
      "Iteration 42, Loss: 351.3917, Δβ: 1.318610\n",
      "Iteration 43, Loss: 347.9977, Δβ: 1.545549\n",
      "Iteration 44, Loss: 345.0720, Δβ: 1.346598\n",
      "Iteration 45, Loss: 336.8546, Δβ: 1.036090\n",
      "Iteration 46, Loss: 334.7261, Δβ: 1.348518\n",
      "Iteration 47, Loss: 335.0626, Δβ: 1.803054\n",
      "Iteration 48, Loss: 336.9188, Δβ: 2.303384\n",
      "Iteration 49, Loss: 342.8856, Δβ: 3.384652\n",
      "Iteration 50, Loss: 358.0633, Δβ: 4.401791\n",
      "Iteration 51, Loss: 367.6776, Δβ: 5.323700\n",
      "Iteration 52, Loss: 378.7845, Δβ: 4.498996\n",
      "Iteration 53, Loss: 331.8114, Δβ: 1.463010\n",
      "Iteration 54, Loss: 321.1422, Δβ: 0.745008\n",
      "Iteration 55, Loss: 317.9317, Δβ: 0.613256\n",
      "Iteration 56, Loss: 317.2331, Δβ: 0.637659\n",
      "Iteration 57, Loss: 316.1382, Δβ: 0.535544\n",
      "Iteration 58, Loss: 315.1615, Δβ: 0.387069\n",
      "Iteration 59, Loss: 314.2039, Δβ: 0.274458\n",
      "Iteration 60, Loss: 313.5277, Δβ: 0.254028\n",
      "Iteration 61, Loss: 312.8974, Δβ: 0.248095\n",
      "Iteration 62, Loss: 312.2859, Δβ: 0.245725\n",
      "Iteration 63, Loss: 311.6843, Δβ: 0.243755\n",
      "Iteration 64, Loss: 311.0916, Δβ: 0.242436\n",
      "Iteration 65, Loss: 310.5051, Δβ: 0.239782\n",
      "Iteration 66, Loss: 309.9299, Δβ: 0.238730\n",
      "Iteration 67, Loss: 309.3607, Δβ: 0.237347\n",
      "Iteration 68, Loss: 308.7976, Δβ: 0.235802\n",
      "Iteration 69, Loss: 308.2414, Δβ: 0.234364\n",
      "Iteration 70, Loss: 307.6917, Δβ: 0.233392\n",
      "Iteration 71, Loss: 307.1473, Δβ: 0.232771\n",
      "Iteration 72, Loss: 306.6057, Δβ: 0.231282\n",
      "Iteration 73, Loss: 306.0703, Δβ: 0.230843\n",
      "Iteration 74, Loss: 305.5378, Δβ: 0.230212\n",
      "Iteration 75, Loss: 305.0076, Δβ: 0.229707\n",
      "Iteration 76, Loss: 304.4799, Δβ: 0.229291\n",
      "Iteration 77, Loss: 303.9543, Δβ: 0.228655\n",
      "Iteration 78, Loss: 303.4317, Δβ: 0.228331\n",
      "Iteration 79, Loss: 302.9103, Δβ: 0.227745\n",
      "Iteration 80, Loss: 302.3915, Δβ: 0.227378\n",
      "Iteration 81, Loss: 301.8745, Δβ: 0.227231\n",
      "Iteration 82, Loss: 301.3583, Δβ: 0.226831\n",
      "Iteration 83, Loss: 300.8436, Δβ: 0.226432\n",
      "Iteration 84, Loss: 300.3308, Δβ: 0.226194\n",
      "Iteration 85, Loss: 299.8191, Δβ: 0.225572\n",
      "Iteration 86, Loss: 299.3101, Δβ: 0.225000\n",
      "Iteration 87, Loss: 298.8035, Δβ: 0.224734\n",
      "Iteration 88, Loss: 298.2985, Δβ: 0.224308\n",
      "Iteration 89, Loss: 297.7948, Δβ: 0.223242\n",
      "Iteration 90, Loss: 297.2963, Δβ: 0.222724\n",
      "Iteration 91, Loss: 296.7998, Δβ: 0.222298\n",
      "Iteration 92, Loss: 296.3053, Δβ: 0.221831\n",
      "Iteration 93, Loss: 295.8129, Δβ: 0.221627\n",
      "Iteration 94, Loss: 295.3218, Δβ: 0.221399\n",
      "Iteration 95, Loss: 294.8314, Δβ: 0.221278\n",
      "Iteration 96, Loss: 294.3419, Δβ: 0.220925\n",
      "Iteration 97, Loss: 293.8536, Δβ: 0.220859\n",
      "Iteration 98, Loss: 293.3658, Δβ: 0.220816\n",
      "Iteration 99, Loss: 292.8783, Δβ: 0.220550\n",
      "Iteration 100, Loss: 292.3918, Δβ: 0.220510\n",
      "Iteration 101, Loss: 291.9056, Δβ: 0.220474\n",
      "Iteration 102, Loss: 291.4195, Δβ: 0.220440\n",
      "Iteration 103, Loss: 290.9336, Δβ: 0.220408\n",
      "Iteration 104, Loss: 290.4479, Δβ: 0.220114\n",
      "Iteration 105, Loss: 289.9634, Δβ: 0.220083\n",
      "Iteration 106, Loss: 289.4790, Δβ: 0.219656\n",
      "Iteration 107, Loss: 288.9962, Δβ: 0.219569\n",
      "Iteration 108, Loss: 288.5142, Δβ: 0.219315\n",
      "Iteration 109, Loss: 288.0331, Δβ: 0.219287\n",
      "Iteration 110, Loss: 287.5523, Δβ: 0.218481\n",
      "Iteration 111, Loss: 287.0743, Δβ: 0.218015\n",
      "Iteration 112, Loss: 286.5989, Δβ: 0.217831\n",
      "Iteration 113, Loss: 286.1242, Δβ: 0.217687\n",
      "Iteration 114, Loss: 285.6503, Δβ: 0.217491\n",
      "Iteration 115, Loss: 285.1770, Δβ: 0.217152\n",
      "Iteration 116, Loss: 284.7051, Δβ: 0.216990\n",
      "Iteration 117, Loss: 284.2343, Δβ: 0.216832\n",
      "Iteration 118, Loss: 283.7640, Δβ: 0.216704\n",
      "Iteration 119, Loss: 283.2944, Δβ: 0.216675\n",
      "Iteration 120, Loss: 282.8250, Δβ: 0.216519\n",
      "Iteration 121, Loss: 282.3559, Δβ: 0.216308\n",
      "Iteration 122, Loss: 281.8881, Δβ: 0.216282\n",
      "Iteration 123, Loss: 281.4203, Δβ: 0.216257\n",
      "Iteration 124, Loss: 280.9527, Δβ: 0.216234\n",
      "Iteration 125, Loss: 280.4851, Δβ: 0.216212\n",
      "Iteration 126, Loss: 280.0177, Δβ: 0.215776\n",
      "Iteration 127, Loss: 279.5516, Δβ: 0.215444\n",
      "Iteration 128, Loss: 279.0874, Δβ: 0.215225\n",
      "Iteration 129, Loss: 278.6242, Δβ: 0.215037\n",
      "Iteration 130, Loss: 278.1617, Δβ: 0.214799\n",
      "Iteration 131, Loss: 277.7000, Δβ: 0.214296\n",
      "Iteration 132, Loss: 277.2407, Δβ: 0.214182\n",
      "Iteration 133, Loss: 276.7819, Δβ: 0.214098\n",
      "Iteration 134, Loss: 276.3236, Δβ: 0.213911\n",
      "Iteration 135, Loss: 275.8659, Δβ: 0.213868\n",
      "Iteration 136, Loss: 275.4085, Δβ: 0.213836\n",
      "Iteration 137, Loss: 274.9513, Δβ: 0.213805\n",
      "Iteration 138, Loss: 274.4942, Δβ: 0.213775\n",
      "Iteration 139, Loss: 274.0373, Δβ: 0.213746\n",
      "Iteration 140, Loss: 273.5804, Δβ: 0.213635\n",
      "Iteration 141, Loss: 273.1239, Δβ: 0.213461\n",
      "Iteration 142, Loss: 272.6683, Δβ: 0.213434\n",
      "Iteration 143, Loss: 272.2128, Δβ: 0.212989\n",
      "Iteration 144, Loss: 271.7587, Δβ: 0.212622\n",
      "Iteration 145, Loss: 271.3066, Δβ: 0.212161\n",
      "Iteration 146, Loss: 270.8561, Δβ: 0.211791\n",
      "Iteration 147, Loss: 270.4072, Δβ: 0.211507\n",
      "Iteration 148, Loss: 269.9599, Δβ: 0.211476\n",
      "Iteration 149, Loss: 269.5127, Δβ: 0.211447\n",
      "Iteration 150, Loss: 269.0656, Δβ: 0.211418\n",
      "Iteration 151, Loss: 268.6187, Δβ: 0.211216\n",
      "Iteration 152, Loss: 268.1723, Δβ: 0.211129\n",
      "Iteration 153, Loss: 267.7266, Δβ: 0.210618\n",
      "Iteration 154, Loss: 267.2830, Δβ: 0.210425\n",
      "Iteration 155, Loss: 266.8402, Δβ: 0.210342\n",
      "Iteration 156, Loss: 266.3978, Δβ: 0.210306\n",
      "Iteration 157, Loss: 265.9555, Δβ: 0.209690\n",
      "Iteration 158, Loss: 265.5157, Δβ: 0.209491\n",
      "Iteration 159, Loss: 265.0766, Δβ: 0.209151\n",
      "Iteration 160, Loss: 264.6391, Δβ: 0.208890\n",
      "Iteration 161, Loss: 264.2026, Δβ: 0.208842\n",
      "Iteration 162, Loss: 263.7665, Δβ: 0.208810\n",
      "Iteration 163, Loss: 263.3305, Δβ: 0.208722\n",
      "Iteration 164, Loss: 262.8949, Δβ: 0.208515\n",
      "Iteration 165, Loss: 262.4600, Δβ: 0.208463\n",
      "Iteration 166, Loss: 262.0254, Δβ: 0.208432\n",
      "Iteration 167, Loss: 261.5910, Δβ: 0.208284\n",
      "Iteration 168, Loss: 261.1571, Δβ: 0.208220\n",
      "Iteration 169, Loss: 260.7236, Δβ: 0.207894\n",
      "Iteration 170, Loss: 260.2910, Δβ: 0.207765\n",
      "Iteration 171, Loss: 259.8594, Δβ: 0.207719\n",
      "Iteration 172, Loss: 259.4280, Δβ: 0.207676\n",
      "Iteration 173, Loss: 258.9967, Δβ: 0.207452\n",
      "Iteration 174, Loss: 258.5662, Δβ: 0.207364\n",
      "Iteration 175, Loss: 258.1362, Δβ: 0.207324\n",
      "Iteration 176, Loss: 257.7064, Δβ: 0.206851\n",
      "Iteration 177, Loss: 257.2781, Δβ: 0.206132\n",
      "Iteration 178, Loss: 256.8529, Δβ: 0.205847\n",
      "Iteration 179, Loss: 256.4290, Δβ: 0.205725\n",
      "Iteration 180, Loss: 256.0058, Δβ: 0.205680\n",
      "Iteration 181, Loss: 255.5828, Δβ: 0.205636\n",
      "Iteration 182, Loss: 255.1600, Δβ: 0.205558\n",
      "Iteration 183, Loss: 254.7375, Δβ: 0.205407\n",
      "Iteration 184, Loss: 254.3156, Δβ: 0.205129\n",
      "Iteration 185, Loss: 253.8947, Δβ: 0.205070\n",
      "Iteration 186, Loss: 253.4742, Δβ: 0.205029\n",
      "Iteration 187, Loss: 253.0538, Δβ: 0.204989\n",
      "Iteration 188, Loss: 252.6337, Δβ: 0.204951\n",
      "Iteration 189, Loss: 252.2137, Δβ: 0.204913\n",
      "Iteration 190, Loss: 251.7938, Δβ: 0.204876\n",
      "Iteration 191, Loss: 251.3741, Δβ: 0.204839\n",
      "Iteration 192, Loss: 250.9546, Δβ: 0.204804\n",
      "Iteration 193, Loss: 250.5351, Δβ: 0.204374\n",
      "Iteration 194, Loss: 250.1171, Δβ: 0.203901\n",
      "Iteration 195, Loss: 249.7008, Δβ: 0.203517\n",
      "Iteration 196, Loss: 249.2866, Δβ: 0.203240\n",
      "Iteration 197, Loss: 248.8736, Δβ: 0.203201\n",
      "Iteration 198, Loss: 248.4607, Δβ: 0.203049\n",
      "Iteration 199, Loss: 248.0482, Δβ: 0.202291\n",
      "Iteration 200, Loss: 247.6388, Δβ: 0.202072\n",
      "Iteration 201, Loss: 247.2305, Δβ: 0.201951\n",
      "Iteration 202, Loss: 246.8226, Δβ: 0.201756\n",
      "Iteration 203, Loss: 246.4154, Δβ: 0.201378\n",
      "Iteration 204, Loss: 246.0099, Δβ: 0.201330\n",
      "Iteration 205, Loss: 245.6046, Δβ: 0.201101\n",
      "Iteration 206, Loss: 245.1999, Δβ: 0.200730\n",
      "Iteration 207, Loss: 244.7970, Δβ: 0.200416\n",
      "Iteration 208, Loss: 244.3951, Δβ: 0.200327\n",
      "Iteration 209, Loss: 243.9938, Δβ: 0.200206\n",
      "Iteration 210, Loss: 243.5929, Δβ: 0.199467\n",
      "Iteration 211, Loss: 243.1945, Δβ: 0.199173\n",
      "Iteration 212, Loss: 242.7976, Δβ: 0.198893\n",
      "Iteration 213, Loss: 242.4020, Δβ: 0.198846\n",
      "Iteration 214, Loss: 242.0066, Δβ: 0.198805\n",
      "Iteration 215, Loss: 241.6114, Δβ: 0.198636\n",
      "Iteration 216, Loss: 241.2169, Δβ: 0.198594\n",
      "Iteration 217, Loss: 240.8225, Δβ: 0.198334\n",
      "Iteration 218, Loss: 240.4290, Δβ: 0.198288\n",
      "Iteration 219, Loss: 240.0359, Δβ: 0.198001\n",
      "Iteration 220, Loss: 239.6437, Δβ: 0.197956\n",
      "Iteration 221, Loss: 239.2519, Δβ: 0.197920\n",
      "Iteration 222, Loss: 238.8602, Δβ: 0.197883\n",
      "Iteration 223, Loss: 238.4687, Δβ: 0.197842\n",
      "Iteration 224, Loss: 238.0773, Δβ: 0.197797\n",
      "Iteration 225, Loss: 237.6861, Δβ: 0.197746\n",
      "Iteration 226, Loss: 237.2951, Δβ: 0.197689\n",
      "Iteration 227, Loss: 236.9044, Δβ: 0.197625\n",
      "Iteration 228, Loss: 236.5139, Δβ: 0.197558\n",
      "Iteration 229, Loss: 236.1237, Δβ: 0.197491\n",
      "Iteration 230, Loss: 235.7337, Δβ: 0.197429\n",
      "Iteration 231, Loss: 235.3440, Δβ: 0.197375\n",
      "Iteration 232, Loss: 234.9545, Δβ: 0.197329\n",
      "Iteration 233, Loss: 234.5651, Δβ: 0.196997\n",
      "Iteration 234, Loss: 234.1769, Δβ: 0.196959\n",
      "Iteration 235, Loss: 233.7890, Δβ: 0.196888\n",
      "Iteration 236, Loss: 233.4013, Δβ: 0.196695\n",
      "Iteration 237, Loss: 233.0145, Δβ: 0.196664\n",
      "Iteration 238, Loss: 232.6277, Δβ: 0.196336\n",
      "Iteration 239, Loss: 232.2421, Δβ: 0.196023\n",
      "Iteration 240, Loss: 231.8576, Δβ: 0.195775\n",
      "Iteration 241, Loss: 231.4741, Δβ: 0.195490\n",
      "Iteration 242, Loss: 231.0917, Δβ: 0.195387\n",
      "Iteration 243, Loss: 230.7100, Δβ: 0.195364\n",
      "Iteration 244, Loss: 230.3283, Δβ: 0.195163\n",
      "Iteration 245, Loss: 229.9474, Δβ: 0.195113\n",
      "Iteration 246, Loss: 229.5666, Δβ: 0.194919\n",
      "Iteration 247, Loss: 229.1867, Δβ: 0.194879\n",
      "Iteration 248, Loss: 228.8070, Δβ: 0.194569\n",
      "Iteration 249, Loss: 228.4283, Δβ: 0.194292\n",
      "Iteration 250, Loss: 228.0506, Δβ: 0.194145\n",
      "Iteration 251, Loss: 227.6735, Δβ: 0.194012\n",
      "Iteration 252, Loss: 227.2972, Δβ: 0.193748\n",
      "Iteration 253, Loss: 226.9215, Δβ: 0.193670\n",
      "Iteration 254, Loss: 226.5464, Δβ: 0.193649\n",
      "Iteration 255, Loss: 226.1715, Δβ: 0.193629\n",
      "Iteration 256, Loss: 225.7966, Δβ: 0.193609\n",
      "Iteration 257, Loss: 225.4217, Δβ: 0.193590\n",
      "Iteration 258, Loss: 225.0470, Δβ: 0.193456\n",
      "Iteration 259, Loss: 224.6726, Δβ: 0.192939\n",
      "Iteration 260, Loss: 224.3000, Δβ: 0.192762\n",
      "Iteration 261, Loss: 223.9283, Δβ: 0.192619\n",
      "Iteration 262, Loss: 223.5573, Δβ: 0.192595\n",
      "Iteration 263, Loss: 223.1864, Δβ: 0.192571\n",
      "Iteration 264, Loss: 222.8156, Δβ: 0.192546\n",
      "Iteration 265, Loss: 222.4448, Δβ: 0.192520\n",
      "Iteration 266, Loss: 222.0742, Δβ: 0.192315\n",
      "Iteration 267, Loss: 221.7041, Δβ: 0.192172\n",
      "Iteration 268, Loss: 221.3349, Δβ: 0.192100\n",
      "Iteration 269, Loss: 220.9658, Δβ: 0.191872\n",
      "Iteration 270, Loss: 220.5976, Δβ: 0.191558\n",
      "Iteration 271, Loss: 220.2307, Δβ: 0.191503\n",
      "Iteration 272, Loss: 219.8640, Δβ: 0.191359\n",
      "Iteration 273, Loss: 219.4978, Δβ: 0.191227\n",
      "Iteration 274, Loss: 219.1321, Δβ: 0.190943\n",
      "Iteration 275, Loss: 218.7676, Δβ: 0.190863\n",
      "Iteration 276, Loss: 218.4034, Δβ: 0.190796\n",
      "Iteration 277, Loss: 218.0394, Δβ: 0.190537\n",
      "Iteration 278, Loss: 217.6762, Δβ: 0.190422\n",
      "Iteration 279, Loss: 217.3136, Δβ: 0.190356\n",
      "Iteration 280, Loss: 216.9514, Δβ: 0.190293\n",
      "Iteration 281, Loss: 216.5893, Δβ: 0.189976\n",
      "Iteration 282, Loss: 216.2283, Δβ: 0.189906\n",
      "Iteration 283, Loss: 215.8677, Δβ: 0.189852\n",
      "Iteration 284, Loss: 215.5073, Δβ: 0.189805\n",
      "Iteration 285, Loss: 215.1471, Δβ: 0.189764\n",
      "Iteration 286, Loss: 214.7870, Δβ: 0.189730\n",
      "Iteration 287, Loss: 214.4271, Δβ: 0.189700\n",
      "Iteration 288, Loss: 214.0672, Δβ: 0.189674\n",
      "Iteration 289, Loss: 213.7075, Δβ: 0.189651\n",
      "Iteration 290, Loss: 213.3478, Δβ: 0.189437\n",
      "Iteration 291, Loss: 212.9888, Δβ: 0.189152\n",
      "Iteration 292, Loss: 212.6310, Δβ: 0.189119\n",
      "Iteration 293, Loss: 212.2734, Δβ: 0.189091\n",
      "Iteration 294, Loss: 211.9158, Δβ: 0.189066\n",
      "Iteration 295, Loss: 211.5584, Δβ: 0.189043\n",
      "Iteration 296, Loss: 211.2011, Δβ: 0.188733\n",
      "Iteration 297, Loss: 210.8446, Δβ: 0.188684\n",
      "Iteration 298, Loss: 210.4886, Δβ: 0.188662\n",
      "Iteration 299, Loss: 210.1327, Δβ: 0.188641\n",
      "Iteration 300, Loss: 209.7769, Δβ: 0.188621\n",
      "Iteration 301, Loss: 209.4211, Δβ: 0.188601\n",
      "Iteration 302, Loss: 209.0654, Δβ: 0.188582\n",
      "Iteration 303, Loss: 208.7098, Δβ: 0.188563\n",
      "Iteration 304, Loss: 208.3543, Δβ: 0.188328\n",
      "Iteration 305, Loss: 207.9995, Δβ: 0.188303\n",
      "Iteration 306, Loss: 207.6449, Δβ: 0.188209\n",
      "Iteration 307, Loss: 207.2906, Δβ: 0.188088\n",
      "Iteration 308, Loss: 206.9369, Δβ: 0.187874\n",
      "Iteration 309, Loss: 206.5837, Δβ: 0.187771\n",
      "Iteration 310, Loss: 206.2311, Δβ: 0.187747\n",
      "Iteration 311, Loss: 205.8787, Δβ: 0.187725\n",
      "Iteration 312, Loss: 205.5263, Δβ: 0.187703\n",
      "Iteration 313, Loss: 205.1740, Δβ: 0.187583\n",
      "Iteration 314, Loss: 204.8220, Δβ: 0.187437\n",
      "Iteration 315, Loss: 204.4707, Δβ: 0.187407\n",
      "Iteration 316, Loss: 204.1195, Δβ: 0.187381\n",
      "Iteration 317, Loss: 203.7684, Δβ: 0.187357\n",
      "Iteration 318, Loss: 203.4174, Δβ: 0.187333\n",
      "Iteration 319, Loss: 203.0665, Δβ: 0.187111\n",
      "Iteration 320, Loss: 202.7163, Δβ: 0.187085\n",
      "Iteration 321, Loss: 202.3663, Δβ: 0.187063\n",
      "Iteration 322, Loss: 202.0164, Δβ: 0.187041\n",
      "Iteration 323, Loss: 201.6666, Δβ: 0.186404\n",
      "Iteration 324, Loss: 201.3187, Δβ: 0.186220\n",
      "Iteration 325, Loss: 200.9718, Δβ: 0.185778\n",
      "Iteration 326, Loss: 200.6267, Δβ: 0.185732\n",
      "Iteration 327, Loss: 200.2817, Δβ: 0.185694\n",
      "Iteration 328, Loss: 199.9369, Δβ: 0.185559\n",
      "Iteration 329, Loss: 199.5925, Δβ: 0.185018\n",
      "Iteration 330, Loss: 199.2501, Δβ: 0.184950\n",
      "Iteration 331, Loss: 198.9081, Δβ: 0.184613\n",
      "Iteration 332, Loss: 198.5672, Δβ: 0.184405\n",
      "Iteration 333, Loss: 198.2270, Δβ: 0.184315\n",
      "Iteration 334, Loss: 197.8873, Δβ: 0.184273\n",
      "Iteration 335, Loss: 197.5478, Δβ: 0.184235\n",
      "Iteration 336, Loss: 197.2084, Δβ: 0.184199\n",
      "Iteration 337, Loss: 196.8691, Δβ: 0.183964\n",
      "Iteration 338, Loss: 196.5305, Δβ: 0.183890\n",
      "Iteration 339, Loss: 196.1924, Δβ: 0.183807\n",
      "Iteration 340, Loss: 195.8545, Δβ: 0.183508\n",
      "Iteration 341, Loss: 195.5177, Δβ: 0.183307\n",
      "Iteration 342, Loss: 195.1815, Δβ: 0.183217\n",
      "Iteration 343, Loss: 194.8458, Δβ: 0.183188\n",
      "Iteration 344, Loss: 194.5102, Δβ: 0.183161\n",
      "Iteration 345, Loss: 194.1748, Δβ: 0.182865\n",
      "Iteration 346, Loss: 193.8401, Δβ: 0.182745\n",
      "Iteration 347, Loss: 193.5061, Δβ: 0.182720\n",
      "Iteration 348, Loss: 193.1723, Δβ: 0.182604\n",
      "Iteration 349, Loss: 192.8388, Δβ: 0.182335\n",
      "Iteration 350, Loss: 192.5061, Δβ: 0.182181\n",
      "Iteration 351, Loss: 192.1742, Δβ: 0.182147\n",
      "Iteration 352, Loss: 191.8424, Δβ: 0.182115\n",
      "Iteration 353, Loss: 191.5108, Δβ: 0.182084\n",
      "Iteration 354, Loss: 191.1793, Δβ: 0.182049\n",
      "Iteration 355, Loss: 190.8479, Δβ: 0.181711\n",
      "Iteration 356, Loss: 190.5175, Δβ: 0.181622\n",
      "Iteration 357, Loss: 190.1877, Δβ: 0.181587\n",
      "Iteration 358, Loss: 189.8580, Δβ: 0.181553\n",
      "Iteration 359, Loss: 189.5284, Δβ: 0.181448\n",
      "Iteration 360, Loss: 189.1991, Δβ: 0.181368\n",
      "Iteration 361, Loss: 188.8702, Δβ: 0.181328\n",
      "Iteration 362, Loss: 188.5414, Δβ: 0.181292\n",
      "Iteration 363, Loss: 188.2128, Δβ: 0.181257\n",
      "Iteration 364, Loss: 187.8843, Δβ: 0.181221\n",
      "Iteration 365, Loss: 187.5559, Δβ: 0.181184\n",
      "Iteration 366, Loss: 187.2276, Δβ: 0.181147\n",
      "Iteration 367, Loss: 186.8995, Δβ: 0.180792\n",
      "Iteration 368, Loss: 186.5724, Δβ: 0.180441\n",
      "Iteration 369, Loss: 186.2468, Δβ: 0.180399\n",
      "Iteration 370, Loss: 185.9214, Δβ: 0.179904\n",
      "Iteration 371, Loss: 185.5972, Δβ: 0.179402\n",
      "Iteration 372, Loss: 185.2754, Δβ: 0.179352\n",
      "Iteration 373, Loss: 184.9538, Δβ: 0.179302\n",
      "Iteration 374, Loss: 184.6323, Δβ: 0.179223\n",
      "Iteration 375, Loss: 184.3111, Δβ: 0.179148\n",
      "Iteration 376, Loss: 183.9902, Δβ: 0.179097\n",
      "Iteration 377, Loss: 183.6695, Δβ: 0.178934\n",
      "Iteration 378, Loss: 183.3492, Δβ: 0.178645\n",
      "Iteration 379, Loss: 183.0299, Δβ: 0.178415\n",
      "Iteration 380, Loss: 182.7117, Δβ: 0.178256\n",
      "Iteration 381, Loss: 182.3938, Δβ: 0.177957\n",
      "Iteration 382, Loss: 182.0770, Δβ: 0.177704\n",
      "Iteration 383, Loss: 181.7611, Δβ: 0.177398\n",
      "Iteration 384, Loss: 181.4462, Δβ: 0.177275\n",
      "Iteration 385, Loss: 181.1320, Δβ: 0.177223\n",
      "Iteration 386, Loss: 180.8180, Δβ: 0.177174\n",
      "Iteration 387, Loss: 180.5041, Δβ: 0.177128\n",
      "Iteration 388, Loss: 180.1904, Δβ: 0.177085\n",
      "Iteration 389, Loss: 179.8768, Δβ: 0.177044\n",
      "Iteration 390, Loss: 179.5634, Δβ: 0.177005\n",
      "Iteration 391, Loss: 179.2502, Δβ: 0.176840\n",
      "Iteration 392, Loss: 178.9374, Δβ: 0.176792\n",
      "Iteration 393, Loss: 178.6248, Δβ: 0.176527\n",
      "Iteration 394, Loss: 178.3130, Δβ: 0.176201\n",
      "Iteration 395, Loss: 178.0025, Δβ: 0.176145\n",
      "Iteration 396, Loss: 177.6923, Δβ: 0.175807\n",
      "Iteration 397, Loss: 177.3829, Δβ: 0.175496\n",
      "Iteration 398, Loss: 177.0750, Δβ: 0.175441\n",
      "Iteration 399, Loss: 176.7672, Δβ: 0.175393\n",
      "Iteration 400, Loss: 176.4596, Δβ: 0.175350\n",
      "Iteration 401, Loss: 176.1522, Δβ: 0.175308\n",
      "Iteration 402, Loss: 175.8449, Δβ: 0.175269\n",
      "Iteration 403, Loss: 175.5377, Δβ: 0.175137\n",
      "Iteration 404, Loss: 175.2309, Δβ: 0.175067\n",
      "Iteration 405, Loss: 174.9245, Δβ: 0.175031\n",
      "Iteration 406, Loss: 174.6181, Δβ: 0.174902\n",
      "Iteration 407, Loss: 174.3123, Δβ: 0.174782\n",
      "Iteration 408, Loss: 174.0067, Δβ: 0.174680\n",
      "Iteration 409, Loss: 173.7016, Δβ: 0.174643\n",
      "Iteration 410, Loss: 173.3966, Δβ: 0.174607\n",
      "Iteration 411, Loss: 173.0918, Δβ: 0.174192\n",
      "Iteration 412, Loss: 172.7883, Δβ: 0.174155\n",
      "Iteration 413, Loss: 172.4850, Δβ: 0.174121\n",
      "Iteration 414, Loss: 172.1819, Δβ: 0.174038\n",
      "Iteration 415, Loss: 171.8789, Δβ: 0.173849\n",
      "Iteration 416, Loss: 171.5767, Δβ: 0.173817\n",
      "Iteration 417, Loss: 171.2746, Δβ: 0.173433\n",
      "Iteration 418, Loss: 170.9735, Δβ: 0.173237\n",
      "Iteration 419, Loss: 170.6734, Δβ: 0.173197\n",
      "Iteration 420, Loss: 170.3735, Δβ: 0.173158\n",
      "Iteration 421, Loss: 170.0737, Δβ: 0.172930\n",
      "Iteration 422, Loss: 169.7744, Δβ: 0.172625\n",
      "Iteration 423, Loss: 169.4764, Δβ: 0.172580\n",
      "Iteration 424, Loss: 169.1786, Δβ: 0.172540\n",
      "Iteration 425, Loss: 168.8810, Δβ: 0.172233\n",
      "Iteration 426, Loss: 168.5843, Δβ: 0.172194\n",
      "Iteration 427, Loss: 168.2878, Δβ: 0.172159\n",
      "Iteration 428, Loss: 167.9914, Δβ: 0.171832\n",
      "Iteration 429, Loss: 167.6961, Δβ: 0.171767\n",
      "Iteration 430, Loss: 167.4011, Δβ: 0.171453\n",
      "Iteration 431, Loss: 167.1070, Δβ: 0.171402\n",
      "Iteration 432, Loss: 166.8133, Δβ: 0.171363\n",
      "Iteration 433, Loss: 166.5197, Δβ: 0.171083\n",
      "Iteration 434, Loss: 166.2269, Δβ: 0.171043\n",
      "Iteration 435, Loss: 165.9344, Δβ: 0.171008\n",
      "Iteration 436, Loss: 165.6420, Δβ: 0.170975\n",
      "Iteration 437, Loss: 165.3497, Δβ: 0.170942\n",
      "Iteration 438, Loss: 165.0575, Δβ: 0.170910\n",
      "Iteration 439, Loss: 164.7654, Δβ: 0.170878\n",
      "Iteration 440, Loss: 164.4735, Δβ: 0.170847\n",
      "Iteration 441, Loss: 164.1816, Δβ: 0.170817\n",
      "Iteration 442, Loss: 163.8898, Δβ: 0.170787\n",
      "Iteration 443, Loss: 163.5982, Δβ: 0.170757\n",
      "Iteration 444, Loss: 163.3066, Δβ: 0.170727\n",
      "Iteration 445, Loss: 163.0152, Δβ: 0.170384\n",
      "Iteration 446, Loss: 162.7247, Δβ: 0.169836\n",
      "Iteration 447, Loss: 162.4359, Δβ: 0.169723\n",
      "Iteration 448, Loss: 162.1478, Δβ: 0.169670\n",
      "Iteration 449, Loss: 161.8600, Δβ: 0.169198\n",
      "Iteration 450, Loss: 161.5733, Δβ: 0.168825\n",
      "Iteration 451, Loss: 161.2882, Δβ: 0.168674\n",
      "Iteration 452, Loss: 161.0037, Δβ: 0.168616\n",
      "Iteration 453, Loss: 160.7194, Δβ: 0.168435\n",
      "Iteration 454, Loss: 160.4356, Δβ: 0.168249\n",
      "Iteration 455, Loss: 160.1526, Δβ: 0.168200\n",
      "Iteration 456, Loss: 159.8697, Δβ: 0.168013\n",
      "Iteration 457, Loss: 159.5873, Δβ: 0.167914\n",
      "Iteration 458, Loss: 159.3053, Δβ: 0.167879\n",
      "Iteration 459, Loss: 159.0235, Δβ: 0.167832\n",
      "Iteration 460, Loss: 158.7419, Δβ: 0.167664\n",
      "Iteration 461, Loss: 158.4608, Δβ: 0.167632\n",
      "Iteration 462, Loss: 158.1798, Δβ: 0.167600\n",
      "Iteration 463, Loss: 157.8989, Δβ: 0.167570\n",
      "Iteration 464, Loss: 157.6182, Δβ: 0.167047\n",
      "Iteration 465, Loss: 157.3389, Δβ: 0.166906\n",
      "Iteration 466, Loss: 157.0602, Δβ: 0.166522\n",
      "Iteration 467, Loss: 156.7827, Δβ: 0.166465\n",
      "Iteration 468, Loss: 156.5056, Δβ: 0.166190\n",
      "Iteration 469, Loss: 156.2292, Δβ: 0.166106\n",
      "Iteration 470, Loss: 155.9533, Δβ: 0.166078\n",
      "Iteration 471, Loss: 155.6775, Δβ: 0.165838\n",
      "Iteration 472, Loss: 155.4024, Δβ: 0.165768\n",
      "Iteration 473, Loss: 155.1276, Δβ: 0.165534\n",
      "Iteration 474, Loss: 154.8536, Δβ: 0.165497\n",
      "Iteration 475, Loss: 154.5798, Δβ: 0.165249\n",
      "Iteration 476, Loss: 154.3065, Δβ: 0.165116\n",
      "Iteration 477, Loss: 154.0339, Δβ: 0.165083\n",
      "Iteration 478, Loss: 153.7614, Δβ: 0.164914\n",
      "Iteration 479, Loss: 153.4892, Δβ: 0.164387\n",
      "Iteration 480, Loss: 153.2190, Δβ: 0.164051\n",
      "Iteration 481, Loss: 152.9499, Δβ: 0.164016\n",
      "Iteration 482, Loss: 152.6809, Δβ: 0.163982\n",
      "Iteration 483, Loss: 152.4121, Δβ: 0.163949\n",
      "Iteration 484, Loss: 152.1433, Δβ: 0.163915\n",
      "Iteration 485, Loss: 151.8746, Δβ: 0.163881\n",
      "Iteration 486, Loss: 151.6061, Δβ: 0.163555\n",
      "Iteration 487, Loss: 151.3383, Δβ: 0.163417\n",
      "Iteration 488, Loss: 151.0713, Δβ: 0.163378\n",
      "Iteration 489, Loss: 150.8044, Δβ: 0.163339\n",
      "Iteration 490, Loss: 150.5376, Δβ: 0.163299\n",
      "Iteration 491, Loss: 150.2710, Δβ: 0.163259\n",
      "Iteration 492, Loss: 150.0045, Δβ: 0.163219\n",
      "Iteration 493, Loss: 149.7381, Δβ: 0.163178\n",
      "Iteration 494, Loss: 149.4719, Δβ: 0.163085\n",
      "Iteration 495, Loss: 149.2059, Δβ: 0.162962\n",
      "Iteration 496, Loss: 148.9404, Δβ: 0.162913\n",
      "Iteration 497, Loss: 148.6750, Δβ: 0.162866\n",
      "Iteration 498, Loss: 148.4098, Δβ: 0.162819\n",
      "Iteration 499, Loss: 148.1447, Δβ: 0.162774\n",
      "Iteration 500, Loss: 147.8798, Δβ: 0.162618\n",
      "Iteration 501, Loss: 147.6154, Δβ: 0.162439\n",
      "Iteration 502, Loss: 147.3514, Δβ: 0.162186\n",
      "Iteration 503, Loss: 147.0883, Δβ: 0.162073\n",
      "Iteration 504, Loss: 146.8257, Δβ: 0.162024\n",
      "Iteration 505, Loss: 146.5632, Δβ: 0.161942\n",
      "Iteration 506, Loss: 146.3010, Δβ: 0.161899\n",
      "Iteration 507, Loss: 146.0389, Δβ: 0.161859\n",
      "Iteration 508, Loss: 145.7770, Δβ: 0.161821\n",
      "Iteration 509, Loss: 145.5151, Δβ: 0.161640\n",
      "Iteration 510, Loss: 145.2537, Δβ: 0.161508\n",
      "Iteration 511, Loss: 144.9929, Δβ: 0.161469\n",
      "Iteration 512, Loss: 144.7322, Δβ: 0.161046\n",
      "Iteration 513, Loss: 144.4728, Δβ: 0.161009\n",
      "Iteration 514, Loss: 144.2136, Δβ: 0.160599\n",
      "Iteration 515, Loss: 143.9553, Δβ: 0.160312\n",
      "Iteration 516, Loss: 143.6984, Δβ: 0.160003\n",
      "Iteration 517, Loss: 143.4422, Δβ: 0.159917\n",
      "Iteration 518, Loss: 143.1865, Δβ: 0.159557\n",
      "Iteration 519, Loss: 142.9316, Δβ: 0.159467\n",
      "Iteration 520, Loss: 142.6774, Δβ: 0.159431\n",
      "Iteration 521, Loss: 142.4232, Δβ: 0.159396\n",
      "Iteration 522, Loss: 142.1692, Δβ: 0.159359\n",
      "Iteration 523, Loss: 141.9152, Δβ: 0.159280\n",
      "Iteration 524, Loss: 141.6616, Δβ: 0.159247\n",
      "Iteration 525, Loss: 141.4080, Δβ: 0.159214\n",
      "Iteration 526, Loss: 141.1545, Δβ: 0.159182\n",
      "Iteration 527, Loss: 140.9012, Δβ: 0.159150\n",
      "Iteration 528, Loss: 140.6479, Δβ: 0.159119\n",
      "Iteration 529, Loss: 140.3947, Δβ: 0.159088\n",
      "Iteration 530, Loss: 140.1417, Δβ: 0.159057\n",
      "Iteration 531, Loss: 139.8887, Δβ: 0.159026\n",
      "Iteration 532, Loss: 139.6358, Δβ: 0.158876\n",
      "Iteration 533, Loss: 139.3833, Δβ: 0.158715\n",
      "Iteration 534, Loss: 139.1314, Δβ: 0.158685\n",
      "Iteration 535, Loss: 138.8796, Δβ: 0.158592\n",
      "Iteration 536, Loss: 138.6281, Δβ: 0.158452\n",
      "Iteration 537, Loss: 138.3769, Δβ: 0.158390\n",
      "Iteration 538, Loss: 138.1261, Δβ: 0.158355\n",
      "Iteration 539, Loss: 137.8754, Δβ: 0.158320\n",
      "Iteration 540, Loss: 137.6247, Δβ: 0.158286\n",
      "Iteration 541, Loss: 137.3742, Δβ: 0.158106\n",
      "Iteration 542, Loss: 137.1241, Δβ: 0.157974\n",
      "Iteration 543, Loss: 136.8746, Δβ: 0.157936\n",
      "Iteration 544, Loss: 136.6251, Δβ: 0.157900\n",
      "Iteration 545, Loss: 136.3759, Δβ: 0.157864\n",
      "Iteration 546, Loss: 136.1267, Δβ: 0.157771\n",
      "Iteration 547, Loss: 135.8777, Δβ: 0.157727\n",
      "Iteration 548, Loss: 135.6290, Δβ: 0.157500\n",
      "Iteration 549, Loss: 135.3807, Δβ: 0.157368\n",
      "Iteration 550, Loss: 135.1331, Δβ: 0.157010\n",
      "Iteration 551, Loss: 134.8864, Δβ: 0.156807\n",
      "Iteration 552, Loss: 134.6404, Δβ: 0.156288\n",
      "Iteration 553, Loss: 134.3959, Δβ: 0.156184\n",
      "Iteration 554, Loss: 134.1520, Δβ: 0.155819\n",
      "Iteration 555, Loss: 133.9092, Δβ: 0.155321\n",
      "Iteration 556, Loss: 133.6679, Δβ: 0.155043\n",
      "Iteration 557, Loss: 133.4275, Δβ: 0.154988\n",
      "Iteration 558, Loss: 133.1874, Δβ: 0.154566\n",
      "Iteration 559, Loss: 132.9485, Δβ: 0.154514\n",
      "Iteration 560, Loss: 132.7097, Δβ: 0.154464\n",
      "Iteration 561, Loss: 132.4712, Δβ: 0.154415\n",
      "Iteration 562, Loss: 132.2328, Δβ: 0.154368\n",
      "Iteration 563, Loss: 131.9945, Δβ: 0.154313\n",
      "Iteration 564, Loss: 131.7564, Δβ: 0.154237\n",
      "Iteration 565, Loss: 131.5186, Δβ: 0.154191\n",
      "Iteration 566, Loss: 131.2809, Δβ: 0.154039\n",
      "Iteration 567, Loss: 131.0435, Δβ: 0.153819\n",
      "Iteration 568, Loss: 130.8068, Δβ: 0.153630\n",
      "Iteration 569, Loss: 130.5708, Δβ: 0.153340\n",
      "Iteration 570, Loss: 130.3356, Δβ: 0.153233\n",
      "Iteration 571, Loss: 130.1008, Δβ: 0.153179\n",
      "Iteration 572, Loss: 129.8662, Δβ: 0.153128\n",
      "Iteration 573, Loss: 129.6318, Δβ: 0.153077\n",
      "Iteration 574, Loss: 129.3975, Δβ: 0.153027\n",
      "Iteration 575, Loss: 129.1633, Δβ: 0.152881\n",
      "Iteration 576, Loss: 128.9295, Δβ: 0.152787\n",
      "Iteration 577, Loss: 128.6961, Δβ: 0.152735\n",
      "Iteration 578, Loss: 128.4629, Δβ: 0.152685\n",
      "Iteration 579, Loss: 128.2298, Δβ: 0.152636\n",
      "Iteration 580, Loss: 127.9969, Δβ: 0.152177\n",
      "Iteration 581, Loss: 127.7651, Δβ: 0.152108\n",
      "Iteration 582, Loss: 127.5338, Δβ: 0.151919\n",
      "Iteration 583, Loss: 127.3030, Δβ: 0.151857\n",
      "Iteration 584, Loss: 127.0724, Δβ: 0.151800\n",
      "Iteration 585, Loss: 126.8420, Δβ: 0.151747\n",
      "Iteration 586, Loss: 126.6118, Δβ: 0.151696\n",
      "Iteration 587, Loss: 126.3817, Δβ: 0.151646\n",
      "Iteration 588, Loss: 126.1518, Δβ: 0.151598\n",
      "Iteration 589, Loss: 125.9220, Δβ: 0.151551\n",
      "Iteration 590, Loss: 125.6924, Δβ: 0.151241\n",
      "Iteration 591, Loss: 125.4634, Δβ: 0.151019\n",
      "Iteration 592, Loss: 125.2352, Δβ: 0.150893\n",
      "Iteration 593, Loss: 125.0076, Δβ: 0.150670\n",
      "Iteration 594, Loss: 124.7804, Δβ: 0.150511\n",
      "Iteration 595, Loss: 124.5539, Δβ: 0.150462\n",
      "Iteration 596, Loss: 124.3276, Δβ: 0.150415\n",
      "Iteration 597, Loss: 124.1014, Δβ: 0.150286\n",
      "Iteration 598, Loss: 123.8754, Δβ: 0.149793\n",
      "Iteration 599, Loss: 123.6508, Δβ: 0.149354\n",
      "Iteration 600, Loss: 123.4277, Δβ: 0.148888\n",
      "Iteration 601, Loss: 123.2060, Δβ: 0.148822\n",
      "Iteration 602, Loss: 122.9846, Δβ: 0.148762\n",
      "Iteration 603, Loss: 122.7633, Δβ: 0.148707\n",
      "Iteration 604, Loss: 122.5422, Δβ: 0.148655\n",
      "Iteration 605, Loss: 122.3213, Δβ: 0.147987\n",
      "Iteration 606, Loss: 122.1019, Δβ: 0.147841\n",
      "Iteration 607, Loss: 121.8833, Δβ: 0.147774\n",
      "Iteration 608, Loss: 121.6650, Δβ: 0.147713\n",
      "Iteration 609, Loss: 121.4469, Δβ: 0.147656\n",
      "Iteration 610, Loss: 121.2289, Δβ: 0.147603\n",
      "Iteration 611, Loss: 121.0111, Δβ: 0.147421\n",
      "Iteration 612, Loss: 120.7936, Δβ: 0.147250\n",
      "Iteration 613, Loss: 120.5768, Δβ: 0.147205\n",
      "Iteration 614, Loss: 120.3602, Δβ: 0.147162\n",
      "Iteration 615, Loss: 120.1436, Δβ: 0.147122\n",
      "Iteration 616, Loss: 119.9272, Δβ: 0.147083\n",
      "Iteration 617, Loss: 119.7109, Δβ: 0.147046\n",
      "Iteration 618, Loss: 119.4947, Δβ: 0.147010\n",
      "Iteration 619, Loss: 119.2786, Δβ: 0.146975\n",
      "Iteration 620, Loss: 119.0626, Δβ: 0.146903\n",
      "Iteration 621, Loss: 118.8468, Δβ: 0.146861\n",
      "Iteration 622, Loss: 118.6311, Δβ: 0.146830\n",
      "Iteration 623, Loss: 118.4156, Δβ: 0.146573\n",
      "Iteration 624, Loss: 118.2006, Δβ: 0.146506\n",
      "Iteration 625, Loss: 117.9860, Δβ: 0.146469\n",
      "Iteration 626, Loss: 117.7715, Δβ: 0.146434\n",
      "Iteration 627, Loss: 117.5571, Δβ: 0.146401\n",
      "Iteration 628, Loss: 117.3427, Δβ: 0.146179\n",
      "Iteration 629, Loss: 117.1289, Δβ: 0.146069\n",
      "Iteration 630, Loss: 116.9156, Δβ: 0.146036\n",
      "Iteration 631, Loss: 116.7023, Δβ: 0.146004\n",
      "Iteration 632, Loss: 116.4892, Δβ: 0.145973\n",
      "Iteration 633, Loss: 116.2761, Δβ: 0.145943\n",
      "Iteration 634, Loss: 116.0631, Δβ: 0.145913\n",
      "Iteration 635, Loss: 115.8503, Δβ: 0.145884\n",
      "Iteration 636, Loss: 115.6375, Δβ: 0.145856\n",
      "Iteration 637, Loss: 115.4247, Δβ: 0.145828\n",
      "Iteration 638, Loss: 115.2121, Δβ: 0.145800\n",
      "Iteration 639, Loss: 114.9995, Δβ: 0.145714\n",
      "Iteration 640, Loss: 114.7872, Δβ: 0.145644\n",
      "Iteration 641, Loss: 114.5751, Δβ: 0.145612\n",
      "Iteration 642, Loss: 114.3631, Δβ: 0.145584\n",
      "Iteration 643, Loss: 114.1511, Δβ: 0.145557\n",
      "Iteration 644, Loss: 113.9393, Δβ: 0.145531\n",
      "Iteration 645, Loss: 113.7275, Δβ: 0.145282\n",
      "Iteration 646, Loss: 113.5162, Δβ: 0.145147\n",
      "Iteration 647, Loss: 113.3056, Δβ: 0.145121\n",
      "Iteration 648, Loss: 113.0950, Δβ: 0.145095\n",
      "Iteration 649, Loss: 112.8845, Δβ: 0.144942\n",
      "Iteration 650, Loss: 112.6743, Δβ: 0.144727\n",
      "Iteration 651, Loss: 112.4648, Δβ: 0.144693\n",
      "Iteration 652, Loss: 112.2555, Δβ: 0.144660\n",
      "Iteration 653, Loss: 112.0463, Δβ: 0.144628\n",
      "Iteration 654, Loss: 111.8371, Δβ: 0.144417\n",
      "Iteration 655, Loss: 111.6285, Δβ: 0.144368\n",
      "Iteration 656, Loss: 111.4201, Δβ: 0.144212\n",
      "Iteration 657, Loss: 111.2120, Δβ: 0.144119\n",
      "Iteration 658, Loss: 111.0044, Δβ: 0.143981\n",
      "Iteration 659, Loss: 110.7971, Δβ: 0.143939\n",
      "Iteration 660, Loss: 110.5899, Δβ: 0.143901\n",
      "Iteration 661, Loss: 110.3829, Δβ: 0.143734\n",
      "Iteration 662, Loss: 110.1762, Δβ: 0.143677\n",
      "Iteration 663, Loss: 109.9698, Δβ: 0.143635\n",
      "Iteration 664, Loss: 109.7635, Δβ: 0.143597\n",
      "Iteration 665, Loss: 109.5574, Δβ: 0.143121\n",
      "Iteration 666, Loss: 109.3525, Δβ: 0.143058\n",
      "Iteration 667, Loss: 109.1478, Δβ: 0.143006\n",
      "Iteration 668, Loss: 108.9434, Δβ: 0.142960\n",
      "Iteration 669, Loss: 108.7390, Δβ: 0.142916\n",
      "Iteration 670, Loss: 108.5348, Δβ: 0.142873\n",
      "Iteration 671, Loss: 108.3307, Δβ: 0.142833\n",
      "Iteration 672, Loss: 108.1267, Δβ: 0.142691\n",
      "Iteration 673, Loss: 107.9230, Δβ: 0.142621\n",
      "Iteration 674, Loss: 107.7197, Δβ: 0.142576\n",
      "Iteration 675, Loss: 107.5164, Δβ: 0.142533\n",
      "Iteration 676, Loss: 107.3133, Δβ: 0.142394\n",
      "Iteration 677, Loss: 107.1104, Δβ: 0.142221\n",
      "Iteration 678, Loss: 106.9082, Δβ: 0.142180\n",
      "Iteration 679, Loss: 106.7061, Δβ: 0.142137\n",
      "Iteration 680, Loss: 106.5041, Δβ: 0.142096\n",
      "Iteration 681, Loss: 106.3022, Δβ: 0.142055\n",
      "Iteration 682, Loss: 106.1004, Δβ: 0.142015\n",
      "Iteration 683, Loss: 105.8988, Δβ: 0.141975\n",
      "Iteration 684, Loss: 105.6972, Δβ: 0.141879\n",
      "Iteration 685, Loss: 105.4959, Δβ: 0.141736\n",
      "Iteration 686, Loss: 105.2950, Δβ: 0.141501\n",
      "Iteration 687, Loss: 105.0948, Δβ: 0.141455\n",
      "Iteration 688, Loss: 104.8947, Δβ: 0.141410\n",
      "Iteration 689, Loss: 104.6948, Δβ: 0.141366\n",
      "Iteration 690, Loss: 104.4950, Δβ: 0.141322\n",
      "Iteration 691, Loss: 104.2953, Δβ: 0.141278\n",
      "Iteration 692, Loss: 104.0957, Δβ: 0.141220\n",
      "Iteration 693, Loss: 103.8963, Δβ: 0.141165\n",
      "Iteration 694, Loss: 103.6971, Δβ: 0.140843\n",
      "Iteration 695, Loss: 103.4987, Δβ: 0.140794\n",
      "Iteration 696, Loss: 103.3005, Δβ: 0.140746\n",
      "Iteration 697, Loss: 103.1025, Δβ: 0.140699\n",
      "Iteration 698, Loss: 102.9045, Δβ: 0.140652\n",
      "Iteration 699, Loss: 102.7067, Δβ: 0.140606\n",
      "Iteration 700, Loss: 102.5091, Δβ: 0.140325\n",
      "Iteration 701, Loss: 102.3121, Δβ: 0.140264\n",
      "Iteration 702, Loss: 102.1154, Δβ: 0.140213\n",
      "Iteration 703, Loss: 101.9188, Δβ: 0.140164\n",
      "Iteration 704, Loss: 101.7224, Δβ: 0.139734\n",
      "Iteration 705, Loss: 101.5272, Δβ: 0.139682\n",
      "Iteration 706, Loss: 101.3321, Δβ: 0.139632\n",
      "Iteration 707, Loss: 101.1372, Δβ: 0.139583\n",
      "Iteration 708, Loss: 100.9424, Δβ: 0.139535\n",
      "Iteration 709, Loss: 100.7477, Δβ: 0.139445\n",
      "Iteration 710, Loss: 100.5532, Δβ: 0.138979\n",
      "Iteration 711, Loss: 100.3601, Δβ: 0.138932\n",
      "Iteration 712, Loss: 100.1671, Δβ: 0.138884\n",
      "Iteration 713, Loss: 99.9743, Δβ: 0.138837\n",
      "Iteration 714, Loss: 99.7815, Δβ: 0.138192\n",
      "Iteration 715, Loss: 99.5903, Δβ: 0.138073\n",
      "Iteration 716, Loss: 99.3997, Δβ: 0.137999\n",
      "Iteration 717, Loss: 99.2093, Δβ: 0.137934\n",
      "Iteration 718, Loss: 99.0191, Δβ: 0.137765\n",
      "Iteration 719, Loss: 98.8293, Δβ: 0.137660\n",
      "Iteration 720, Loss: 98.6398, Δβ: 0.137105\n",
      "Iteration 721, Loss: 98.4517, Δβ: 0.137025\n",
      "Iteration 722, Loss: 98.2640, Δβ: 0.136522\n",
      "Iteration 723, Loss: 98.0774, Δβ: 0.136426\n",
      "Iteration 724, Loss: 97.8914, Δβ: 0.136355\n",
      "Iteration 725, Loss: 97.7055, Δβ: 0.136286\n",
      "Iteration 726, Loss: 97.5198, Δβ: 0.136219\n",
      "Iteration 727, Loss: 97.3343, Δβ: 0.135875\n",
      "Iteration 728, Loss: 97.1497, Δβ: 0.135810\n",
      "Iteration 729, Loss: 96.9653, Δβ: 0.135746\n",
      "Iteration 730, Loss: 96.7811, Δβ: 0.135683\n",
      "Iteration 731, Loss: 96.5970, Δβ: 0.135558\n",
      "Iteration 732, Loss: 96.4133, Δβ: 0.135494\n",
      "Iteration 733, Loss: 96.2297, Δβ: 0.135154\n",
      "Iteration 734, Loss: 96.0469, Δβ: 0.135017\n",
      "Iteration 735, Loss: 95.8646, Δβ: 0.134928\n",
      "Iteration 736, Loss: 95.6826, Δβ: 0.134861\n",
      "Iteration 737, Loss: 95.5008, Δβ: 0.134798\n",
      "Iteration 738, Loss: 95.3191, Δβ: 0.134736\n",
      "Iteration 739, Loss: 95.1376, Δβ: 0.134675\n",
      "Iteration 740, Loss: 94.9563, Δβ: 0.134614\n",
      "Iteration 741, Loss: 94.7751, Δβ: 0.134555\n",
      "Iteration 742, Loss: 94.5941, Δβ: 0.134496\n",
      "Iteration 743, Loss: 94.4132, Δβ: 0.134394\n",
      "Iteration 744, Loss: 94.2326, Δβ: 0.134245\n",
      "Iteration 745, Loss: 94.0524, Δβ: 0.134051\n",
      "Iteration 746, Loss: 93.8727, Δβ: 0.133970\n",
      "Iteration 747, Loss: 93.6932, Δβ: 0.133807\n",
      "Iteration 748, Loss: 93.5142, Δβ: 0.133607\n",
      "Iteration 749, Loss: 93.3357, Δβ: 0.133532\n",
      "Iteration 750, Loss: 93.1574, Δβ: 0.133465\n",
      "Iteration 751, Loss: 92.9794, Δβ: 0.133399\n",
      "Iteration 752, Loss: 92.8015, Δβ: 0.133335\n",
      "Iteration 753, Loss: 92.6237, Δβ: 0.133271\n",
      "Iteration 754, Loss: 92.4461, Δβ: 0.133209\n",
      "Iteration 755, Loss: 92.2687, Δβ: 0.133146\n",
      "Iteration 756, Loss: 92.0915, Δβ: 0.133085\n",
      "Iteration 757, Loss: 91.9144, Δβ: 0.133023\n",
      "Iteration 758, Loss: 91.7375, Δβ: 0.132962\n",
      "Iteration 759, Loss: 91.5608, Δβ: 0.132900\n",
      "Iteration 760, Loss: 91.3842, Δβ: 0.132839\n",
      "Iteration 761, Loss: 91.2078, Δβ: 0.132778\n",
      "Iteration 762, Loss: 91.0315, Δβ: 0.132717\n",
      "Iteration 763, Loss: 90.8554, Δβ: 0.132656\n",
      "Iteration 764, Loss: 90.6795, Δβ: 0.132594\n",
      "Iteration 765, Loss: 90.5037, Δβ: 0.132210\n",
      "Iteration 766, Loss: 90.3289, Δβ: 0.132130\n",
      "Iteration 767, Loss: 90.1543, Δβ: 0.132056\n",
      "Iteration 768, Loss: 89.9800, Δβ: 0.131986\n",
      "Iteration 769, Loss: 89.8058, Δβ: 0.131761\n",
      "Iteration 770, Loss: 89.6323, Δβ: 0.131684\n",
      "Iteration 771, Loss: 89.4589, Δβ: 0.131612\n",
      "Iteration 772, Loss: 89.2857, Δβ: 0.131464\n",
      "Iteration 773, Loss: 89.1130, Δβ: 0.131385\n",
      "Iteration 774, Loss: 88.9404, Δβ: 0.131308\n",
      "Iteration 775, Loss: 88.7680, Δβ: 0.131232\n",
      "Iteration 776, Loss: 88.5958, Δβ: 0.131157\n",
      "Iteration 777, Loss: 88.4239, Δβ: 0.131083\n",
      "Iteration 778, Loss: 88.2521, Δβ: 0.131008\n",
      "Iteration 779, Loss: 88.0805, Δβ: 0.130934\n",
      "Iteration 780, Loss: 87.9091, Δβ: 0.130859\n",
      "Iteration 781, Loss: 87.7379, Δβ: 0.130784\n",
      "Iteration 782, Loss: 87.5669, Δβ: 0.130709\n",
      "Iteration 783, Loss: 87.3961, Δβ: 0.130473\n",
      "Iteration 784, Loss: 87.2258, Δβ: 0.130321\n",
      "Iteration 785, Loss: 87.0560, Δβ: 0.130244\n",
      "Iteration 786, Loss: 86.8864, Δβ: 0.130166\n",
      "Iteration 787, Loss: 86.7171, Δβ: 0.130088\n",
      "Iteration 788, Loss: 86.5479, Δβ: 0.130010\n",
      "Iteration 789, Loss: 86.3789, Δβ: 0.129859\n",
      "Iteration 790, Loss: 86.2102, Δβ: 0.129648\n",
      "Iteration 791, Loss: 86.0422, Δβ: 0.129559\n",
      "Iteration 792, Loss: 85.8744, Δβ: 0.129111\n",
      "Iteration 793, Loss: 85.7074, Δβ: 0.128616\n",
      "Iteration 794, Loss: 85.5420, Δβ: 0.128518\n",
      "Iteration 795, Loss: 85.3769, Δβ: 0.128422\n",
      "Iteration 796, Loss: 85.2121, Δβ: 0.128293\n",
      "Iteration 797, Loss: 85.0475, Δβ: 0.127924\n",
      "Iteration 798, Loss: 84.8837, Δβ: 0.127780\n",
      "Iteration 799, Loss: 84.7205, Δβ: 0.127633\n",
      "Iteration 800, Loss: 84.5576, Δβ: 0.127475\n",
      "Iteration 801, Loss: 84.3951, Δβ: 0.127386\n",
      "Iteration 802, Loss: 84.2329, Δβ: 0.127301\n",
      "Iteration 803, Loss: 84.0709, Δβ: 0.127219\n",
      "Iteration 804, Loss: 83.9091, Δβ: 0.127138\n",
      "Iteration 805, Loss: 83.7475, Δβ: 0.127053\n",
      "Iteration 806, Loss: 83.5861, Δβ: 0.126957\n",
      "Iteration 807, Loss: 83.4250, Δβ: 0.126499\n",
      "Iteration 808, Loss: 83.2649, Δβ: 0.126414\n",
      "Iteration 809, Loss: 83.1052, Δβ: 0.126338\n",
      "Iteration 810, Loss: 82.9456, Δβ: 0.126262\n",
      "Iteration 811, Loss: 82.7862, Δβ: 0.126188\n",
      "Iteration 812, Loss: 82.6270, Δβ: 0.126114\n",
      "Iteration 813, Loss: 82.4680, Δβ: 0.125793\n",
      "Iteration 814, Loss: 82.3097, Δβ: 0.125696\n",
      "Iteration 815, Loss: 82.1518, Δβ: 0.125619\n",
      "Iteration 816, Loss: 81.9940, Δβ: 0.125544\n",
      "Iteration 817, Loss: 81.8365, Δβ: 0.125470\n",
      "Iteration 818, Loss: 81.6791, Δβ: 0.125396\n",
      "Iteration 819, Loss: 81.5219, Δβ: 0.125323\n",
      "Iteration 820, Loss: 81.3649, Δβ: 0.125250\n",
      "Iteration 821, Loss: 81.2081, Δβ: 0.125178\n",
      "Iteration 822, Loss: 81.0514, Δβ: 0.125106\n",
      "Iteration 823, Loss: 80.8949, Δβ: 0.125034\n",
      "Iteration 824, Loss: 80.7387, Δβ: 0.124722\n",
      "Iteration 825, Loss: 80.5830, Δβ: 0.124588\n",
      "Iteration 826, Loss: 80.4278, Δβ: 0.124148\n",
      "Iteration 827, Loss: 80.2734, Δβ: 0.123828\n",
      "Iteration 828, Loss: 80.1201, Δβ: 0.123729\n",
      "Iteration 829, Loss: 79.9671, Δβ: 0.123637\n",
      "Iteration 830, Loss: 79.8143, Δβ: 0.123549\n",
      "Iteration 831, Loss: 79.6617, Δβ: 0.123465\n",
      "Iteration 832, Loss: 79.5093, Δβ: 0.123383\n",
      "Iteration 833, Loss: 79.3571, Δβ: 0.123303\n",
      "Iteration 834, Loss: 79.2051, Δβ: 0.123224\n",
      "Iteration 835, Loss: 79.0533, Δβ: 0.122886\n",
      "Iteration 836, Loss: 78.9023, Δβ: 0.122785\n",
      "Iteration 837, Loss: 78.7515, Δβ: 0.122701\n",
      "Iteration 838, Loss: 78.6010, Δβ: 0.122618\n",
      "Iteration 839, Loss: 78.4507, Δβ: 0.122234\n",
      "Iteration 840, Loss: 78.3012, Δβ: 0.122131\n",
      "Iteration 841, Loss: 78.1521, Δβ: 0.122044\n",
      "Iteration 842, Loss: 78.0032, Δβ: 0.121958\n",
      "Iteration 843, Loss: 77.8545, Δβ: 0.121873\n",
      "Iteration 844, Loss: 77.7061, Δβ: 0.121788\n",
      "Iteration 845, Loss: 77.5578, Δβ: 0.121187\n",
      "Iteration 846, Loss: 77.4109, Δβ: 0.121005\n",
      "Iteration 847, Loss: 77.2645, Δβ: 0.120898\n",
      "Iteration 848, Loss: 77.1184, Δβ: 0.120794\n",
      "Iteration 849, Loss: 76.9726, Δβ: 0.120692\n",
      "Iteration 850, Loss: 76.8270, Δβ: 0.120590\n",
      "Iteration 851, Loss: 76.6816, Δβ: 0.120490\n",
      "Iteration 852, Loss: 76.5365, Δβ: 0.120390\n",
      "Iteration 853, Loss: 76.3916, Δβ: 0.120290\n",
      "Iteration 854, Loss: 76.2470, Δβ: 0.120191\n",
      "Iteration 855, Loss: 76.1026, Δβ: 0.120091\n",
      "Iteration 856, Loss: 75.9584, Δβ: 0.119992\n",
      "Iteration 857, Loss: 75.8145, Δβ: 0.119892\n",
      "Iteration 858, Loss: 75.6708, Δβ: 0.119793\n",
      "Iteration 859, Loss: 75.5274, Δβ: 0.119682\n",
      "Iteration 860, Loss: 75.3842, Δβ: 0.119581\n",
      "Iteration 861, Loss: 75.2413, Δβ: 0.119481\n",
      "Iteration 862, Loss: 75.0986, Δβ: 0.119381\n",
      "Iteration 863, Loss: 74.9561, Δβ: 0.119282\n",
      "Iteration 864, Loss: 74.8139, Δβ: 0.119182\n",
      "Iteration 865, Loss: 74.6719, Δβ: 0.119084\n",
      "Iteration 866, Loss: 74.5301, Δβ: 0.118986\n",
      "Iteration 867, Loss: 74.3886, Δβ: 0.118888\n",
      "Iteration 868, Loss: 74.2473, Δβ: 0.118791\n",
      "Iteration 869, Loss: 74.1063, Δβ: 0.118696\n",
      "Iteration 870, Loss: 73.9655, Δβ: 0.118601\n",
      "Iteration 871, Loss: 73.8249, Δβ: 0.118421\n",
      "Iteration 872, Loss: 73.6847, Δβ: 0.118330\n",
      "Iteration 873, Loss: 73.5447, Δβ: 0.118145\n",
      "Iteration 874, Loss: 73.4051, Δβ: 0.117862\n",
      "Iteration 875, Loss: 73.2662, Δβ: 0.117768\n",
      "Iteration 876, Loss: 73.1276, Δβ: 0.117630\n",
      "Iteration 877, Loss: 72.9892, Δβ: 0.117540\n",
      "Iteration 878, Loss: 72.8511, Δβ: 0.117213\n",
      "Iteration 879, Loss: 72.7135, Δβ: 0.116855\n",
      "Iteration 880, Loss: 72.5771, Δβ: 0.116759\n",
      "Iteration 881, Loss: 72.4408, Δβ: 0.116669\n",
      "Iteration 882, Loss: 72.3047, Δβ: 0.116583\n",
      "Iteration 883, Loss: 72.1688, Δβ: 0.116034\n",
      "Iteration 884, Loss: 72.0341, Δβ: 0.115768\n",
      "Iteration 885, Loss: 71.9001, Δβ: 0.115599\n",
      "Iteration 886, Loss: 71.7664, Δβ: 0.115150\n",
      "Iteration 887, Loss: 71.6339, Δβ: 0.115045\n",
      "Iteration 888, Loss: 71.5016, Δβ: 0.114950\n",
      "Iteration 889, Loss: 71.3695, Δβ: 0.114862\n",
      "Iteration 890, Loss: 71.2376, Δβ: 0.114780\n",
      "Iteration 891, Loss: 71.1059, Δβ: 0.114701\n",
      "Iteration 892, Loss: 70.9744, Δβ: 0.114624\n",
      "Iteration 893, Loss: 70.8431, Δβ: 0.114550\n",
      "Iteration 894, Loss: 70.7119, Δβ: 0.114478\n",
      "Iteration 895, Loss: 70.5809, Δβ: 0.114408\n",
      "Iteration 896, Loss: 70.4500, Δβ: 0.114336\n",
      "Iteration 897, Loss: 70.3193, Δβ: 0.114267\n",
      "Iteration 898, Loss: 70.1888, Δβ: 0.114141\n",
      "Iteration 899, Loss: 70.0585, Δβ: 0.114065\n",
      "Iteration 900, Loss: 69.9285, Δβ: 0.113995\n",
      "Iteration 901, Loss: 69.7986, Δβ: 0.113927\n",
      "Iteration 902, Loss: 69.6688, Δβ: 0.113860\n",
      "Iteration 903, Loss: 69.5392, Δβ: 0.113794\n",
      "Iteration 904, Loss: 69.4098, Δβ: 0.113729\n",
      "Iteration 905, Loss: 69.2805, Δβ: 0.113665\n",
      "Iteration 906, Loss: 69.1513, Δβ: 0.113602\n",
      "Iteration 907, Loss: 69.0223, Δβ: 0.113539\n",
      "Iteration 908, Loss: 68.8934, Δβ: 0.113476\n",
      "Iteration 909, Loss: 68.7647, Δβ: 0.113414\n",
      "Iteration 910, Loss: 68.6361, Δβ: 0.113352\n",
      "Iteration 911, Loss: 68.5076, Δβ: 0.113290\n",
      "Iteration 912, Loss: 68.3793, Δβ: 0.112692\n",
      "Iteration 913, Loss: 68.2522, Δβ: 0.112552\n",
      "Iteration 914, Loss: 68.1256, Δβ: 0.112442\n",
      "Iteration 915, Loss: 67.9993, Δβ: 0.112321\n",
      "Iteration 916, Loss: 67.8731, Δβ: 0.112030\n",
      "Iteration 917, Loss: 67.7477, Δβ: 0.111912\n",
      "Iteration 918, Loss: 67.6225, Δβ: 0.111813\n",
      "Iteration 919, Loss: 67.4975, Δβ: 0.111720\n",
      "Iteration 920, Loss: 67.3728, Δβ: 0.111629\n",
      "Iteration 921, Loss: 67.2482, Δβ: 0.111541\n",
      "Iteration 922, Loss: 67.1238, Δβ: 0.111455\n",
      "Iteration 923, Loss: 66.9997, Δβ: 0.111356\n",
      "Iteration 924, Loss: 66.8757, Δβ: 0.111233\n",
      "Iteration 925, Loss: 66.7520, Δβ: 0.111143\n",
      "Iteration 926, Loss: 66.6286, Δβ: 0.111055\n",
      "Iteration 927, Loss: 66.5053, Δβ: 0.110969\n",
      "Iteration 928, Loss: 66.3822, Δβ: 0.110885\n",
      "Iteration 929, Loss: 66.2593, Δβ: 0.110801\n",
      "Iteration 930, Loss: 66.1365, Δβ: 0.110712\n",
      "Iteration 931, Loss: 66.0140, Δβ: 0.110600\n",
      "Iteration 932, Loss: 65.8917, Δβ: 0.110517\n",
      "Iteration 933, Loss: 65.7696, Δβ: 0.110434\n",
      "Iteration 934, Loss: 65.6477, Δβ: 0.110352\n",
      "Iteration 935, Loss: 65.5260, Δβ: 0.110270\n",
      "Iteration 936, Loss: 65.4044, Δβ: 0.110189\n",
      "Iteration 937, Loss: 65.2831, Δβ: 0.110109\n",
      "Iteration 938, Loss: 65.1619, Δβ: 0.110029\n",
      "Iteration 939, Loss: 65.0409, Δβ: 0.109949\n",
      "Iteration 940, Loss: 64.9200, Δβ: 0.109502\n",
      "Iteration 941, Loss: 64.8000, Δβ: 0.109395\n",
      "Iteration 942, Loss: 64.6804, Δβ: 0.109302\n",
      "Iteration 943, Loss: 64.5610, Δβ: 0.109210\n",
      "Iteration 944, Loss: 64.4418, Δβ: 0.109120\n",
      "Iteration 945, Loss: 64.3227, Δβ: 0.109030\n",
      "Iteration 946, Loss: 64.2039, Δβ: 0.108940\n",
      "Iteration 947, Loss: 64.0853, Δβ: 0.108835\n",
      "Iteration 948, Loss: 63.9669, Δβ: 0.108716\n",
      "Iteration 949, Loss: 63.8487, Δβ: 0.108625\n",
      "Iteration 950, Loss: 63.7308, Δβ: 0.108535\n",
      "Iteration 951, Loss: 63.6130, Δβ: 0.108446\n",
      "Iteration 952, Loss: 63.4955, Δβ: 0.108357\n",
      "Iteration 953, Loss: 63.3781, Δβ: 0.108231\n",
      "Iteration 954, Loss: 63.2610, Δβ: 0.107904\n",
      "Iteration 955, Loss: 63.1446, Δβ: 0.107815\n",
      "Iteration 956, Loss: 63.0284, Δβ: 0.107728\n",
      "Iteration 957, Loss: 62.9124, Δβ: 0.107618\n",
      "Iteration 958, Loss: 62.7966, Δβ: 0.107425\n",
      "Iteration 959, Loss: 62.6813, Δβ: 0.107335\n",
      "Iteration 960, Loss: 62.5661, Δβ: 0.107239\n",
      "Iteration 961, Loss: 62.4511, Δβ: 0.107089\n",
      "Iteration 962, Loss: 62.3365, Δβ: 0.107001\n",
      "Iteration 963, Loss: 62.2220, Δβ: 0.106915\n",
      "Iteration 964, Loss: 62.1078, Δβ: 0.106801\n",
      "Iteration 965, Loss: 61.9937, Δβ: 0.106692\n",
      "Iteration 966, Loss: 61.8799, Δβ: 0.106529\n",
      "Iteration 967, Loss: 61.7665, Δβ: 0.106441\n",
      "Iteration 968, Loss: 61.6532, Δβ: 0.106358\n",
      "Iteration 969, Loss: 61.5402, Δβ: 0.106275\n",
      "Iteration 970, Loss: 61.4273, Δβ: 0.106193\n",
      "Iteration 971, Loss: 61.3145, Δβ: 0.106111\n",
      "Iteration 972, Loss: 61.2020, Δβ: 0.106029\n",
      "Iteration 973, Loss: 61.0896, Δβ: 0.105947\n",
      "Iteration 974, Loss: 60.9774, Δβ: 0.105865\n",
      "Iteration 975, Loss: 60.8654, Δβ: 0.105783\n",
      "Iteration 976, Loss: 60.7535, Δβ: 0.105701\n",
      "Iteration 977, Loss: 60.6418, Δβ: 0.105618\n",
      "Iteration 978, Loss: 60.5303, Δβ: 0.105535\n",
      "Iteration 979, Loss: 60.4190, Δβ: 0.105452\n",
      "Iteration 980, Loss: 60.3078, Δβ: 0.105369\n",
      "Iteration 981, Loss: 60.1968, Δβ: 0.105237\n",
      "Iteration 982, Loss: 60.0861, Δβ: 0.104750\n",
      "Iteration 983, Loss: 59.9763, Δβ: 0.104350\n",
      "Iteration 984, Loss: 59.8674, Δβ: 0.104158\n",
      "Iteration 985, Loss: 59.7589, Δβ: 0.104043\n",
      "Iteration 986, Loss: 59.6507, Δβ: 0.103938\n",
      "Iteration 987, Loss: 59.5428, Δβ: 0.103837\n",
      "Iteration 988, Loss: 59.4350, Δβ: 0.103738\n",
      "Iteration 989, Loss: 59.3274, Δβ: 0.103551\n",
      "Iteration 990, Loss: 59.2202, Δβ: 0.103233\n",
      "Iteration 991, Loss: 59.1137, Δβ: 0.103113\n",
      "Iteration 992, Loss: 59.0074, Δβ: 0.103007\n",
      "Iteration 993, Loss: 58.9013, Δβ: 0.102852\n",
      "Iteration 994, Loss: 58.7956, Δβ: 0.102676\n",
      "Iteration 995, Loss: 58.6902, Δβ: 0.102562\n",
      "Iteration 996, Loss: 58.5851, Δβ: 0.102320\n",
      "Iteration 997, Loss: 58.4804, Δβ: 0.102202\n",
      "Iteration 998, Loss: 58.3760, Δβ: 0.102090\n",
      "Iteration 999, Loss: 58.2718, Δβ: 0.101983\n",
      "Iteration 1000, Loss: 58.1679, Δβ: 0.101878\n",
      "Test Accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "# check the model with MNIST dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load and process data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X = StandardScaler().fit_transform(X)  # standardize\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# train the model\n",
    "model = CustomProxGradSolver()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict and check the accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_2=model.predict(X_train)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
